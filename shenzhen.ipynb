{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5f49cc-1ca4-47ed-9943-fae3287d32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from model.HGNN import HGNN\n",
    "from utils.dataset import JobDataset\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3892844d-a1ca-4dd6-ad2e-9f468101ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # set random seed\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    train_loader = DataLoader(JobDataset(root=\"/individual/xxx/train/sample\",top_k=args.top_k, val_prop=args.val_prop), batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(JobDataset(root=\"/individual/xxx/test/sample\",top_k=args.top_k, mode='test', val_prop=args.val_prop), batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    in_channels = 0\n",
    "    out_channels = args.n_hid\n",
    "    ncount = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        phi1 = data['phi1']\n",
    "        phi1_inv = data['phi1_inverse']\n",
    "        phi2 = data['phi2']\n",
    "        phi2_inv = data['phi2_inverse']\n",
    "        fea = data['Fea']\n",
    "        joblst = data['joblst']\n",
    "        label = data['label']\n",
    "        in_channels = fea.shape[-1]\n",
    "        ncount = phi1.shape[-1]\n",
    "        break\n",
    "\n",
    "\n",
    "    model = HGNN(in_channels, out_channels, ncount, args.device, args.top_k)\n",
    "    if args.device != 'cpu':\n",
    "        model = model.to(args.device)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=args.lr,\n",
    "                                      weight_decay=args.weight_decay)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.stepsize, gamma=args.gamma)\n",
    "\n",
    "    model_path = './save_models/'\n",
    "    log_path = './logs'\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    elif os.path.exists(model_path + 'parameter.pkl'):\n",
    "        model.load_state_dict(torch.load(model_path + 'parameter.pkl'))\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "    writer = tensorboardX.SummaryWriter(log_path)\n",
    "\n",
    "    step_n = 0\n",
    "    best_hr = 0.0\n",
    "    best_mrr = 0.0\n",
    "    for epoch in range(args.n_epoch):\n",
    "#         print(\"epoch is:\", epoch)\n",
    "        model.train()\n",
    "\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "        for batch_idx, data in progress_bar: # tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            phi1 = data['phi1']\n",
    "            phi1_inv = data['phi1_inverse']\n",
    "            phi2 = data['phi2']\n",
    "            phi2_inv = data['phi2_inverse']\n",
    "            fea = data['Fea']\n",
    "            joblst = data['joblst']\n",
    "            label = data['label']\n",
    "            # label = label.unsqueeze(1)\n",
    "\n",
    "            if args.device != 'cpu':\n",
    "                phi1 = phi1.to(args.device)\n",
    "                phi1_inv = phi1_inv.to(args.device)\n",
    "                phi2 = phi2.to(args.device)\n",
    "                phi2_inv = phi2_inv.to(args.device)\n",
    "                fea = fea.to(args.device)\n",
    "                joblst = joblst.to(args.device)\n",
    "                label = label.to(args.device)\n",
    "\n",
    "            output = model.forward(phi1, phi1_inv, phi2, phi2_inv, fea, joblst)\n",
    "            # print('main output shape:', output.shape)\n",
    "            # print('main label shape:', label.shape)\n",
    "            # print(label)\n",
    "            loss = loss_func(output, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, pred = torch.max(output.data, dim=1)\n",
    "            correct = pred.eq(label.data).cpu().sum()\n",
    "\n",
    "            writer.add_scalar(\"train loss\", loss.item(), global_step=step_n)\n",
    "            writer.add_scalar(\"train correct\", 100.0 * correct.item() / args.batch_size, global_step=step_n)\n",
    "            progress_bar.set_description(f\"Epoch {epoch + 1} loss={round(loss.item(), 8)}\")\n",
    "            step_n += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        sum_loss = 0\n",
    "        model.eval()\n",
    "        hit_rates = []\n",
    "        average_precisions = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "        mrrs = []\n",
    "        ndcgs = []\n",
    "        for i, data in enumerate(test_loader):\n",
    "            phi1 = data['phi1']\n",
    "            phi1_inv = data['phi1_inverse']\n",
    "            phi2 = data['phi2']\n",
    "            phi2_inv = data['phi2_inverse']\n",
    "            fea = data['Fea']\n",
    "            joblst = data['joblst']\n",
    "            label = data['label']\n",
    "\n",
    "            if args.device != 'cpu':\n",
    "                phi1 = phi1.to(args.device)\n",
    "                phi1_inv = phi1_inv.to(args.device)\n",
    "                phi2 = phi2.to(args.device)\n",
    "                phi2_inv = phi2_inv.to(args.device)\n",
    "                fea = fea.to(args.device)\n",
    "                joblst = joblst.to(args.device)\n",
    "                label = label.to(args.device)\n",
    "\n",
    "            output = model(phi1, phi1_inv, phi2, phi2_inv, fea, joblst)\n",
    "            loss = loss_func(output, label)\n",
    "            _, pred = torch.max(output.data, dim=1)\n",
    "\n",
    "            for i in range(pred.shape[0]):\n",
    "                recommended_items = pred.tolist()[i]\n",
    "                test_items = label.tolist()[i]\n",
    "                hit_rates.append(hit_rate(recommended_items, test_items))\n",
    "                average_precisions.append(average_precision(recommended_items, test_items))\n",
    "                pre = precision(recommended_items, test_items)\n",
    "                rec = recall(recommended_items, test_items)\n",
    "                precisions.append(pre)\n",
    "                recalls.append(rec)\n",
    "                f1_scores.append(f1_score(pre, rec))\n",
    "                mrrs.append(mean_reciprocal_rank(recommended_items, test_items))\n",
    "                ndcgs.append(ndcg(recommended_items, test_items))\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "        test_loss = sum_loss * 1.0 / len(test_loader)\n",
    "\n",
    "        # 计算总体指标\n",
    "        overall_mrr = np.mean(mrrs)\n",
    "        overall_hr = np.mean(hit_rates)\n",
    "\n",
    "        writer.add_scalar(\"test loss\", test_loss, global_step=epoch + 1)\n",
    "        writer.add_scalar(\"test Mean Reciprocal Rank\", overall_mrr, global_step=epoch + 1)\n",
    "        writer.add_scalar(\"test Hit Rate\", overall_hr, global_step=epoch + 1)\n",
    "        \n",
    "        flag = False\n",
    "        if best_hr < overall_hr:\n",
    "            best_hr = overall_hr\n",
    "            flag = True\n",
    "        if best_mrr < overall_mrr:\n",
    "            best_mrr = overall_mrr\n",
    "            flag = True\n",
    "        \n",
    "        if flag:\n",
    "            print(\"Best Hit Rate is\", best_hr, 'Best Mean Reciprocal Rank:', best_mrr)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd0fed-1d0a-4ed7-b676-1c394ede1047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tDensity of wavelets: 1.43%.\n",
      "\tDensity of inverse wavelets: 8.1%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss=5.64865507: 100%|██████████| 1108/1108 [15:40<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hit Rate is 0.1744421906693712 Best Mean Reciprocal Rank: 0.06414807302231237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss=6.02935263: 100%|██████████| 1108/1108 [00:43<00:00, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hit Rate is 0.3772819472616633 Best Mean Reciprocal Rank: 0.1257606490872211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss=5.96969549: 100%|██████████| 1108/1108 [00:44<00:00, 24.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hit Rate is 0.45841784989858014 Best Mean Reciprocal Rank: 0.22582826233941855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss=6.04860986: 100%|██████████| 1108/1108 [00:40<00:00, 27.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hit Rate is 0.45841784989858014 Best Mean Reciprocal Rank: 0.3772819472616633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss=5.98127523: 100%|██████████| 1108/1108 [00:39<00:00, 28.41it/s]\n",
      "Epoch 6 loss=6.00982582: 100%|██████████| 1108/1108 [00:38<00:00, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hit Rate is 0.45841784989858014 Best Mean Reciprocal Rank: 0.37829614604462475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss=5.91123927: 100%|██████████| 1108/1108 [00:38<00:00, 28.70it/s]\n",
      "Epoch 8 loss=6.09427009: 100%|██████████| 1108/1108 [00:39<00:00, 28.18it/s]\n",
      "Epoch 9 loss=5.91629317: 100%|██████████| 1108/1108 [00:38<00:00, 28.58it/s]\n",
      "Epoch 10 loss=6.14722958: 100%|██████████| 1108/1108 [00:38<00:00, 28.56it/s]\n",
      "Epoch 11 loss=5.98380447: 100%|██████████| 1108/1108 [00:38<00:00, 28.49it/s]\n",
      "Epoch 12 loss=6.23669923: 100%|██████████| 1108/1108 [00:38<00:00, 28.66it/s]\n",
      "Epoch 13 loss=6.24211307: 100%|██████████| 1108/1108 [00:38<00:00, 28.95it/s]\n",
      "Epoch 14 loss=6.12571133: 100%|██████████| 1108/1108 [00:38<00:00, 28.80it/s]\n",
      "Epoch 15 loss=5.97940967: 100%|██████████| 1108/1108 [00:38<00:00, 28.98it/s]\n",
      "Epoch 16 loss=6.14574979: 100%|██████████| 1108/1108 [00:38<00:00, 28.92it/s]\n",
      "Epoch 17 loss=6.17106639: 100%|██████████| 1108/1108 [00:38<00:00, 28.72it/s]\n",
      "Epoch 18 loss=5.94245986: 100%|██████████| 1108/1108 [00:38<00:00, 28.42it/s]\n",
      "Epoch 19 loss=5.97898064: 100%|██████████| 1108/1108 [00:39<00:00, 28.18it/s]\n",
      "Epoch 20 loss=6.22072459: 100%|██████████| 1108/1108 [00:39<00:00, 28.34it/s]\n",
      "Epoch 21 loss=6.05259381: 100%|██████████| 1108/1108 [00:39<00:00, 28.10it/s]\n",
      "Epoch 22 loss=5.95771948: 100%|██████████| 1108/1108 [00:39<00:00, 28.08it/s]\n",
      "Epoch 34 loss=6.10433197: 100%|██████████| 1108/1108 [00:39<00:00, 27.75it/s]\n",
      "Epoch 54 loss=5.8160129: 100%|██████████| 1108/1108 [00:45<00:00, 24.32it/s] \n",
      "Epoch 59 loss=6.27222761: 100%|██████████| 1108/1108 [00:46<00:00, 23.71it/s]\n",
      "Epoch 63 loss=5.91734677: 100%|██████████| 1108/1108 [00:46<00:00, 24.04it/s]\n",
      "Epoch 64 loss=5.94375049: 100%|██████████| 1108/1108 [00:46<00:00, 23.63it/s]\n",
      "Epoch 67 loss=6.06772935: 100%|██████████| 1108/1108 [01:34<00:00, 11.78it/s]\n",
      "Epoch 69 loss=6.21166422: 100%|██████████| 1108/1108 [01:40<00:00, 11.01it/s]\n",
      "Epoch 71 loss=6.00906909: 100%|██████████| 1108/1108 [01:34<00:00, 11.74it/s]\n",
      "Epoch 74 loss=6.06550479: 100%|██████████| 1108/1108 [01:33<00:00, 11.87it/s]\n",
      "Epoch 76 loss=6.01075221: 100%|██████████| 1108/1108 [01:58<00:00,  9.38it/s]\n",
      "Epoch 78 loss=6.03863143: 100%|██████████| 1108/1108 [02:46<00:00,  6.64it/s]\n",
      "Epoch 79 loss=5.91731421: 100%|██████████| 1108/1108 [02:31<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hit Rate is 0.5436105476673428 Best Mean Reciprocal Rank: 0.5420892494929006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81 loss=6.00161751: 100%|██████████| 1108/1108 [02:38<00:00,  6.99it/s]\n",
      "Epoch 82 loss=5.90560639: 100%|██████████| 1108/1108 [02:51<00:00,  6.48it/s]\n",
      "Epoch 84 loss=5.82187541: 100%|██████████| 1108/1108 [02:33<00:00,  7.23it/s]\n",
      "Epoch 85 loss=6.34907659: 100%|██████████| 1108/1108 [02:37<00:00,  7.05it/s]\n",
      "Epoch 87 loss=5.97808762: 100%|██████████| 1108/1108 [02:38<00:00,  6.99it/s]\n",
      "Epoch 88 loss=6.05731471: 100%|██████████| 1108/1108 [02:43<00:00,  6.78it/s]\n",
      "Epoch 90 loss=5.80650802: 100%|██████████| 1108/1108 [02:41<00:00,  6.85it/s]\n",
      "Epoch 91 loss=6.03529525: 100%|██████████| 1108/1108 [02:49<00:00,  6.54it/s]\n",
      "Epoch 93 loss=5.98445715: 100%|██████████| 1108/1108 [02:33<00:00,  7.20it/s]\n",
      "Epoch 96 loss=6.02796704: 100%|██████████| 1108/1108 [02:42<00:00,  6.80it/s]\n",
      "Epoch 98 loss=6.12781037: 100%|██████████| 1108/1108 [02:37<00:00,  7.04it/s]\n",
      "Epoch 99 loss=5.90980066: 100%|██████████| 1108/1108 [02:38<00:00,  7.01it/s]\n",
      "Epoch 101 loss=5.95487928: 100%|██████████| 1108/1108 [02:40<00:00,  6.90it/s]\n",
      "Epoch 102 loss=6.24273921: 100%|██████████| 1108/1108 [02:44<00:00,  6.74it/s]\n",
      "Epoch 105 loss=6.08220811: 100%|██████████| 1108/1108 [02:41<00:00,  6.85it/s]\n",
      "Epoch 108 loss=5.87040203: 100%|██████████| 1108/1108 [02:33<00:00,  7.20it/s]\n",
      "Epoch 111 loss=6.12856673: 100%|██████████| 1108/1108 [02:45<00:00,  6.70it/s]\n",
      "Epoch 114 loss=6.18690611: 100%|██████████| 1108/1108 [02:40<00:00,  6.92it/s]\n",
      "Epoch 117 loss=5.89139217: 100%|██████████| 1108/1108 [02:40<00:00,  6.91it/s]\n",
      "Epoch 119 loss=5.97098093: 100%|██████████| 1108/1108 [02:40<00:00,  6.91it/s]\n",
      "Epoch 124 loss=5.97824632: 100%|██████████| 1108/1108 [02:38<00:00,  7.00it/s]\n",
      "Epoch 127 loss=6.28212127: 100%|██████████| 1108/1108 [02:49<00:00,  6.54it/s]\n",
      "Epoch 129 loss=5.94772382: 100%|██████████| 1108/1108 [02:40<00:00,  6.90it/s]\n",
      "Epoch 130 loss=6.04473366: 100%|██████████| 1108/1108 [02:42<00:00,  6.83it/s]\n",
      "Epoch 133 loss=5.90389665: 100%|██████████| 1108/1108 [02:46<00:00,  6.67it/s]\n",
      "Epoch 135 loss=5.87404765: 100%|██████████| 1108/1108 [02:35<00:00,  7.14it/s]\n",
      "Epoch 136 loss=6.19299782: 100%|██████████| 1108/1108 [02:35<00:00,  7.13it/s]\n",
      "Epoch 139 loss=6.07596896: 100%|██████████| 1108/1108 [02:52<00:00,  6.44it/s]\n",
      "Epoch 142 loss=6.18887693: 100%|██████████| 1108/1108 [02:45<00:00,  6.71it/s]\n",
      "Epoch 145 loss=6.09270907: 100%|██████████| 1108/1108 [02:48<00:00,  6.59it/s]\n",
      "Epoch 146 loss=6.03378798: 100%|██████████| 1108/1108 [02:45<00:00,  6.68it/s]\n",
      "Epoch 149 loss=6.30306568: 100%|██████████| 1108/1108 [02:53<00:00,  6.38it/s]\n",
      "Epoch 150 loss=6.06064618: 100%|██████████| 1108/1108 [02:55<00:00,  6.31it/s]\n",
      "Epoch 152 loss=5.94691671: 100%|██████████| 1108/1108 [02:47<00:00,  6.62it/s]\n",
      "Epoch 153 loss=5.76802686: 100%|██████████| 1108/1108 [02:57<00:00,  6.25it/s]\n",
      "Epoch 156 loss=6.14292403: 100%|██████████| 1108/1108 [02:40<00:00,  6.91it/s]\n",
      "Epoch 159 loss=6.14189103: 100%|██████████| 1108/1108 [02:52<00:00,  6.42it/s]\n",
      "Epoch 160 loss=5.97106155: 100%|██████████| 1108/1108 [02:53<00:00,  6.37it/s]\n",
      "Epoch 163 loss=6.09222344: 100%|██████████| 1108/1108 [02:56<00:00,  6.27it/s]\n",
      "Epoch 166 loss=6.1053782: 100%|██████████| 1108/1108 [02:47<00:00,  6.60it/s] \n",
      "Epoch 167 loss=6.14490314: 100%|██████████| 1108/1108 [02:51<00:00,  6.45it/s]\n",
      "Epoch 169 loss=5.80548437: 100%|██████████| 1108/1108 [02:46<00:00,  6.64it/s]\n",
      "Epoch 170 loss=5.96652154: 100%|██████████| 1108/1108 [02:46<00:00,  6.67it/s]\n",
      "Epoch 173 loss=6.22570025: 100%|██████████| 1108/1108 [02:59<00:00,  6.18it/s]\n",
      "Epoch 177 loss=6.18129: 100%|██████████| 1108/1108 [02:49<00:00,  6.55it/s]   \n",
      "Epoch 180 loss=5.94145597: 100%|██████████| 1108/1108 [02:42<00:00,  6.82it/s]\n",
      "Epoch 183 loss=6.05963591: 100%|██████████| 1108/1108 [02:42<00:00,  6.81it/s]\n",
      "Epoch 184 loss=5.626632: 100%|██████████| 1108/1108 [02:50<00:00,  6.51it/s]  \n",
      "Epoch 187 loss=5.62884741: 100%|██████████| 1108/1108 [02:44<00:00,  6.74it/s]\n",
      "Epoch 190 loss=5.94730323: 100%|██████████| 1108/1108 [02:44<00:00,  6.72it/s]\n",
      "Epoch 191 loss=5.86802493: 100%|██████████| 1108/1108 [02:47<00:00,  6.60it/s]\n",
      "Epoch 194 loss=5.81753835: 100%|██████████| 1108/1108 [02:51<00:00,  6.47it/s]\n",
      "Epoch 197 loss=5.87399989: 100%|██████████| 1108/1108 [02:59<00:00,  6.17it/s]\n",
      "Epoch 201 loss=5.77936106: 100%|██████████| 1108/1108 [02:55<00:00,  6.30it/s]\n",
      "Epoch 204 loss=6.20734806: 100%|██████████| 1108/1108 [02:53<00:00,  6.38it/s]\n",
      "Epoch 207 loss=5.82418653: 100%|██████████| 1108/1108 [02:50<00:00,  6.49it/s]\n",
      "Epoch 208 loss=6.11956651: 100%|██████████| 1108/1108 [02:53<00:00,  6.39it/s]\n",
      "Epoch 211 loss=5.97439583: 100%|██████████| 1108/1108 [02:49<00:00,  6.53it/s]\n",
      "Epoch 214 loss=6.05332305: 100%|██████████| 1108/1108 [02:48<00:00,  6.59it/s]\n",
      "Epoch 217 loss=5.82636374: 100%|██████████| 1108/1108 [02:48<00:00,  6.59it/s]\n",
      "Epoch 218 loss=6.0846276: 100%|██████████| 1108/1108 [02:45<00:00,  6.68it/s] \n",
      "Epoch 220 loss=5.87768613: 100%|██████████| 1108/1108 [02:44<00:00,  6.75it/s]\n",
      "Epoch 221 loss=6.15603762: 100%|██████████| 1108/1108 [02:33<00:00,  7.20it/s]\n",
      "Epoch 224 loss=5.9886499: 100%|██████████| 1108/1108 [02:47<00:00,  6.61it/s] \n",
      "Epoch 227 loss=5.791669: 100%|██████████| 1108/1108 [02:53<00:00,  6.39it/s]  \n",
      "Epoch 229 loss=5.97758652: 100%|██████████| 1108/1108 [02:45<00:00,  6.70it/s]\n",
      "Epoch 230 loss=6.10173395: 100%|██████████| 1108/1108 [02:46<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hit Rate is 0.5598377281947262 Best Mean Reciprocal Rank: 0.5466531440162272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 233 loss=5.73197772: 100%|██████████| 1108/1108 [02:53<00:00,  6.37it/s]\n",
      "Epoch 236 loss=5.89962398: 100%|██████████| 1108/1108 [02:51<00:00,  6.46it/s]\n",
      "Epoch 237 loss=6.19854167: 100%|██████████| 1108/1108 [02:47<00:00,  6.61it/s]\n",
      "Epoch 240 loss=5.99261169: 100%|██████████| 1108/1108 [02:49<00:00,  6.54it/s]\n",
      "Epoch 243 loss=6.31441742: 100%|██████████| 1108/1108 [02:59<00:00,  6.18it/s]\n",
      "Epoch 244 loss=5.990141: 100%|██████████| 1108/1108 [02:50<00:00,  6.50it/s]  \n",
      "Epoch 247 loss=5.95964933: 100%|██████████| 1108/1108 [02:47<00:00,  6.62it/s]\n",
      "Epoch 250 loss=6.06843906: 100%|██████████| 1108/1108 [02:44<00:00,  6.73it/s]\n",
      "Epoch 253 loss=6.02914018: 100%|██████████| 1108/1108 [02:55<00:00,  6.33it/s]\n",
      "Epoch 254 loss=5.84772434: 100%|██████████| 1108/1108 [02:48<00:00,  6.59it/s]\n",
      "Epoch 257 loss=6.02921368: 100%|██████████| 1108/1108 [02:55<00:00,  6.33it/s]\n",
      "Epoch 260 loss=6.09759022: 100%|██████████| 1108/1108 [02:45<00:00,  6.70it/s]\n",
      "Epoch 264 loss=6.0337963: 100%|██████████| 1108/1108 [02:53<00:00,  6.37it/s] \n",
      "Epoch 266 loss=5.71508703: 100%|██████████| 1108/1108 [02:55<00:00,  6.31it/s]\n",
      "Epoch 267 loss=6.06274332: 100%|██████████| 1108/1108 [02:54<00:00,  6.35it/s]\n",
      "Epoch 270 loss=5.79205766: 100%|██████████| 1108/1108 [02:55<00:00,  6.31it/s]\n",
      "Epoch 271 loss=6.0578484: 100%|██████████| 1108/1108 [02:55<00:00,  6.31it/s] \n",
      "Epoch 274 loss=6.07631925: 100%|██████████| 1108/1108 [02:52<00:00,  6.42it/s]\n",
      "Epoch 277 loss=5.90915151: 100%|██████████| 1108/1108 [02:56<00:00,  6.29it/s]\n",
      "Epoch 280 loss=5.76681568: 100%|██████████| 1108/1108 [02:57<00:00,  6.24it/s]\n",
      "Epoch 281 loss=5.90921187: 100%|██████████| 1108/1108 [02:58<00:00,  6.22it/s]\n",
      "Epoch 284 loss=5.57180951: 100%|██████████| 1108/1108 [02:52<00:00,  6.42it/s]\n",
      "Epoch 287 loss=5.88516941: 100%|██████████| 1108/1108 [02:55<00:00,  6.30it/s]\n",
      "Epoch 288 loss=5.47838257: 100%|██████████| 1108/1108 [02:54<00:00,  6.36it/s]\n",
      "Epoch 294 loss=5.92262455: 100%|██████████| 1108/1108 [02:51<00:00,  6.48it/s]\n",
      "Epoch 297 loss=6.01752744: 100%|██████████| 1108/1108 [02:57<00:00,  6.25it/s]\n",
      "Epoch 300 loss=6.13368659: 100%|██████████| 1108/1108 [02:51<00:00,  6.47it/s]\n",
      "Epoch 301 loss=6.08374131: 100%|██████████| 1108/1108 [02:50<00:00,  6.50it/s]\n",
      "Epoch 304 loss=5.80494705: 100%|██████████| 1108/1108 [03:00<00:00,  6.13it/s]\n",
      "Epoch 307 loss=5.87140844: 100%|██████████| 1108/1108 [02:58<00:00,  6.20it/s]\n",
      "Epoch 308 loss=5.90613565: 100%|██████████| 1108/1108 [02:47<00:00,  6.61it/s]\n",
      "Epoch 310 loss=5.89551936: 100%|██████████| 1108/1108 [02:53<00:00,  6.39it/s]\n",
      "Epoch 311 loss=5.81604953: 100%|██████████| 1108/1108 [02:47<00:00,  6.63it/s]\n",
      "Epoch 314 loss=5.97666456: 100%|██████████| 1108/1108 [02:56<00:00,  6.28it/s]\n",
      "Epoch 317 loss=5.59938298: 100%|██████████| 1108/1108 [02:49<00:00,  6.52it/s]\n",
      "Epoch 320 loss=5.62410419: 100%|██████████| 1108/1108 [02:57<00:00,  6.23it/s]\n",
      "Epoch 321 loss=6.02343936: 100%|██████████| 1108/1108 [02:57<00:00,  6.26it/s]\n",
      "Epoch 323 loss=5.97698834: 100%|██████████| 1108/1108 [02:44<00:00,  6.73it/s]\n",
      "Epoch 328 loss=6.02441098: 100%|██████████| 1108/1108 [03:00<00:00,  6.15it/s]\n",
      "Epoch 330 loss=5.75583342: 100%|██████████| 1108/1108 [02:56<00:00,  6.28it/s]\n",
      "Epoch 333 loss=5.96248887: 100%|██████████| 1108/1108 [02:55<00:00,  6.33it/s]\n",
      "Epoch 336 loss=5.99640188: 100%|██████████| 1108/1108 [02:52<00:00,  6.44it/s]\n",
      "Epoch 339 loss=5.8402609: 100%|██████████| 1108/1108 [03:01<00:00,  6.11it/s] \n",
      "Epoch 342 loss=5.91487445: 100%|██████████| 1108/1108 [02:50<00:00,  6.50it/s]\n",
      "Epoch 345 loss=5.8398188: 100%|██████████| 1108/1108 [03:00<00:00,  6.13it/s] \n",
      "Epoch 351 loss=5.66694134: 100%|██████████| 1108/1108 [02:51<00:00,  6.48it/s]\n",
      "Epoch 354 loss=6.01711811: 100%|██████████| 1108/1108 [02:52<00:00,  6.44it/s]\n",
      "Epoch 356 loss=5.73299335: 100%|██████████| 1108/1108 [02:52<00:00,  6.42it/s]\n",
      "Epoch 360 loss=5.61096117: 100%|██████████| 1108/1108 [02:49<00:00,  6.55it/s]\n",
      "Epoch 372 loss=5.80846432: 100%|██████████| 1108/1108 [03:02<00:00,  6.07it/s]\n",
      "Epoch 375 loss=6.20054725: 100%|██████████| 1108/1108 [02:50<00:00,  6.50it/s]\n",
      "Epoch 378 loss=6.02967724: 100%|██████████| 1108/1108 [02:58<00:00,  6.22it/s]\n",
      "Epoch 381 loss=6.02886353: 100%|██████████| 1108/1108 [02:49<00:00,  6.56it/s]\n",
      "Epoch 384 loss=5.75542336: 100%|██████████| 1108/1108 [02:57<00:00,  6.23it/s]\n",
      "Epoch 390 loss=6.17801587: 100%|██████████| 1108/1108 [02:50<00:00,  6.51it/s]\n",
      "Epoch 393 loss=5.89260051: 100%|██████████| 1108/1108 [02:45<00:00,  6.68it/s]\n",
      "Epoch 396 loss=5.94404238: 100%|██████████| 1108/1108 [02:57<00:00,  6.23it/s]\n",
      "Epoch 399 loss=5.86861404: 100%|██████████| 1108/1108 [02:55<00:00,  6.32it/s]\n",
      "Epoch 402 loss=6.15708744: 100%|██████████| 1108/1108 [02:45<00:00,  6.69it/s]\n",
      "Epoch 405 loss=6.10838305: 100%|██████████| 1108/1108 [02:48<00:00,  6.58it/s]\n",
      "Epoch 411 loss=5.64939823: 100%|██████████| 1108/1108 [02:50<00:00,  6.51it/s]\n",
      "Epoch 414 loss=5.8138829: 100%|██████████| 1108/1108 [02:53<00:00,  6.38it/s] \n",
      "Epoch 415 loss=6.09679152: 100%|██████████| 1108/1108 [02:58<00:00,  6.20it/s]\n",
      "Epoch 418 loss=6.16322042: 100%|██████████| 1108/1108 [02:50<00:00,  6.52it/s]\n",
      "Epoch 421 loss=6.01114348: 100%|██████████| 1108/1108 [02:55<00:00,  6.31it/s]\n",
      "Epoch 423 loss=5.68993344: 100%|██████████| 1108/1108 [02:55<00:00,  6.31it/s]\n",
      "Epoch 426 loss=5.86093111: 100%|██████████| 1108/1108 [02:49<00:00,  6.55it/s]\n",
      "Epoch 429 loss=5.91287851: 100%|██████████| 1108/1108 [02:42<00:00,  6.81it/s]\n",
      "Epoch 432 loss=5.97054265: 100%|██████████| 1108/1108 [02:52<00:00,  6.44it/s]\n",
      "Epoch 435 loss=5.85557006: 100%|██████████| 1108/1108 [02:54<00:00,  6.34it/s]\n",
      "Epoch 438 loss=5.82873353: 100%|██████████| 1108/1108 [02:50<00:00,  6.51it/s]\n",
      "Epoch 445 loss=5.77164969: 100%|██████████| 1108/1108 [02:55<00:00,  6.30it/s]\n",
      "Epoch 448 loss=6.02698577: 100%|██████████| 1108/1108 [02:53<00:00,  6.39it/s]\n",
      "Epoch 451 loss=5.57935199: 100%|██████████| 1108/1108 [02:49<00:00,  6.54it/s]\n",
      "Epoch 456 loss=5.83693668: 100%|██████████| 1108/1108 [02:56<00:00,  6.29it/s]\n",
      "Epoch 459 loss=6.25716126: 100%|██████████| 1108/1108 [02:54<00:00,  6.34it/s]\n",
      "Epoch 472 loss=6.18853293: 100%|██████████| 1108/1108 [02:55<00:00,  6.33it/s]\n",
      "Epoch 475 loss=6.11076535: 100%|██████████| 1108/1108 [02:52<00:00,  6.41it/s]\n",
      "Epoch 478 loss=6.01922231: 100%|██████████| 1108/1108 [02:49<00:00,  6.55it/s]\n",
      "Epoch 479 loss=6.17276806: 100%|██████████| 1108/1108 [02:49<00:00,  6.55it/s]\n",
      "Epoch 489 loss=6.02659842: 100%|██████████| 1108/1108 [02:55<00:00,  6.30it/s]\n",
      "Epoch 492 loss=5.83447515: 100%|██████████| 1108/1108 [02:59<00:00,  6.19it/s]\n",
      "Epoch 498 loss=6.13600132: 100%|██████████| 1108/1108 [03:01<00:00,  6.11it/s]\n",
      "Epoch 502 loss=5.85915567: 100%|██████████| 1108/1108 [03:01<00:00,  6.10it/s]\n",
      "Epoch 510 loss=5.76292868: 100%|██████████| 1108/1108 [02:53<00:00,  6.39it/s]\n",
      "Epoch 513 loss=5.76886877: 100%|██████████| 1108/1108 [02:52<00:00,  6.42it/s]\n",
      "Epoch 525 loss=6.18540069: 100%|██████████| 1108/1108 [02:52<00:00,  6.41it/s]\n",
      "Epoch 528 loss=6.05668405: 100%|██████████| 1108/1108 [02:59<00:00,  6.16it/s]\n",
      "Epoch 531 loss=5.93361027: 100%|██████████| 1108/1108 [02:56<00:00,  6.28it/s]\n",
      "Epoch 534 loss=5.72252052: 100%|██████████| 1108/1108 [02:59<00:00,  6.16it/s]\n",
      "Epoch 537 loss=6.26780385: 100%|██████████| 1108/1108 [03:06<00:00,  5.95it/s]\n",
      "Epoch 540 loss=6.33393524: 100%|██████████| 1108/1108 [02:49<00:00,  6.52it/s]\n",
      "Epoch 542 loss=6.06124512: 100%|██████████| 1108/1108 [02:53<00:00,  6.40it/s]\n",
      "Epoch 545 loss=5.74271679: 100%|██████████| 1108/1108 [02:43<00:00,  6.77it/s]\n",
      "Epoch 555 loss=6.14975718: 100%|██████████| 1108/1108 [02:50<00:00,  6.48it/s]\n",
      "Epoch 556 loss=5.59349029:  57%|█████▋    | 636/1108 [01:36<01:11,  6.61it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 557 loss=6.00472047: 100%|██████████| 1108/1108 [02:52<00:00,  6.43it/s]\n",
      "Epoch 558 loss=5.80960081:  89%|████████▉ | 990/1108 [02:43<00:13,  8.72it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 566 loss=6.04673823: 100%|██████████| 1108/1108 [02:51<00:00,  6.47it/s]\n",
      "Epoch 567 loss=5.78532522: 100%|██████████| 1108/1108 [02:56<00:00,  6.28it/s]\n",
      "Epoch 568 loss=6.01981503:  41%|████▏     | 458/1108 [01:04<01:45,  6.17it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 576 loss=6.21876541: 100%|██████████| 1108/1108 [02:55<00:00,  6.33it/s]\n",
      "Epoch 577 loss=6.14748834: 100%|██████████| 1108/1108 [02:51<00:00,  6.45it/s]\n",
      "Epoch 578 loss=5.96484876:  72%|███████▏  | 793/1108 [02:05<01:09,  4.54it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 608 loss=6.01935848: 100%|██████████| 1108/1108 [02:50<00:00,  6.48it/s]\n",
      "Epoch 611 loss=5.91321251: 100%|██████████| 1108/1108 [02:51<00:00,  6.46it/s]\n",
      "Epoch 613 loss=6.12631632: 100%|██████████| 1108/1108 [02:48<00:00,  6.57it/s]\n",
      "Epoch 616 loss=5.92854256: 100%|██████████| 1108/1108 [02:59<00:00,  6.18it/s]\n",
      "Epoch 618 loss=6.19115218: 100%|██████████| 1108/1108 [02:53<00:00,  6.39it/s]\n",
      "Epoch 619 loss=6.32336805: 100%|██████████| 1108/1108 [02:48<00:00,  6.58it/s]\n",
      "Epoch 622 loss=6.12628185: 100%|██████████| 1108/1108 [03:00<00:00,  6.15it/s]\n",
      "Epoch 630 loss=5.81422153: 100%|██████████| 1108/1108 [02:53<00:00,  6.38it/s]\n",
      "Epoch 633 loss=5.95218209: 100%|██████████| 1108/1108 [02:57<00:00,  6.26it/s]\n",
      "Epoch 636 loss=5.66154364: 100%|██████████| 1108/1108 [02:54<00:00,  6.34it/s]\n",
      "Epoch 641 loss=5.98367798: 100%|██████████| 1108/1108 [02:47<00:00,  6.62it/s]\n",
      "Epoch 644 loss=6.28855817: 100%|██████████| 1108/1108 [02:54<00:00,  6.36it/s]\n",
      "Epoch 649 loss=5.90667229: 100%|██████████| 1108/1108 [02:54<00:00,  6.33it/s]\n",
      "Epoch 653 loss=6.11408292: 100%|██████████| 1108/1108 [02:55<00:00,  6.30it/s]\n",
      "Epoch 654 loss=6.00559721: 100%|██████████| 1108/1108 [02:50<00:00,  6.51it/s]\n",
      "Epoch 662 loss=5.81726468: 100%|██████████| 1108/1108 [02:47<00:00,  6.63it/s]\n",
      "Epoch 665 loss=5.8925603: 100%|██████████| 1108/1108 [02:51<00:00,  6.47it/s] \n",
      "Epoch 670 loss=5.7729782: 100%|██████████| 1108/1108 [02:55<00:00,  6.33it/s] \n",
      "Epoch 673 loss=5.79868494: 100%|██████████| 1108/1108 [02:47<00:00,  6.62it/s]\n",
      "Epoch 676 loss=5.80156008: 100%|██████████| 1108/1108 [02:56<00:00,  6.27it/s]\n",
      "Epoch 679 loss=6.05923777: 100%|██████████| 1108/1108 [02:55<00:00,  6.32it/s]\n",
      "Epoch 682 loss=5.95792041: 100%|██████████| 1108/1108 [02:52<00:00,  6.43it/s]\n",
      "Epoch 687 loss=5.93476559: 100%|██████████| 1108/1108 [02:48<00:00,  6.59it/s]\n",
      "Epoch 690 loss=6.03777655: 100%|██████████| 1108/1108 [02:55<00:00,  6.33it/s]\n",
      "Epoch 693 loss=6.10255365: 100%|██████████| 1108/1108 [02:53<00:00,  6.40it/s]\n",
      "Epoch 695 loss=6.1596603: 100%|██████████| 1108/1108 [02:52<00:00,  6.41it/s] \n",
      "Epoch 698 loss=5.98615746: 100%|██████████| 1108/1108 [02:54<00:00,  6.34it/s]\n",
      "Epoch 701 loss=6.12797142: 100%|██████████| 1108/1108 [02:49<00:00,  6.54it/s]\n",
      "Epoch 704 loss=5.9233656: 100%|██████████| 1108/1108 [02:51<00:00,  6.48it/s] \n",
      "Epoch 715 loss=5.7680013: 100%|██████████| 1108/1108 [02:52<00:00,  6.44it/s] \n",
      "Epoch 723 loss=5.8872397: 100%|██████████| 1108/1108 [02:57<00:00,  6.24it/s] \n",
      "Epoch 726 loss=5.85057721: 100%|██████████| 1108/1108 [02:57<00:00,  6.23it/s]\n",
      "Epoch 731 loss=6.01819148: 100%|██████████| 1108/1108 [02:59<00:00,  6.16it/s]\n",
      "Epoch 734 loss=5.61301626: 100%|██████████| 1108/1108 [02:43<00:00,  6.78it/s]\n",
      "Epoch 742 loss=5.88314198: 100%|██████████| 1108/1108 [02:47<00:00,  6.60it/s]\n",
      "Epoch 745 loss=5.90997968: 100%|██████████| 1108/1108 [02:58<00:00,  6.21it/s]\n",
      "Epoch 751 loss=6.08125523:  86%|████████▌ | 950/1108 [01:30<00:09, 16.97it/s]"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='test')\n",
    "    parser.add_argument('--model', type=str, default='MKGNN')\n",
    "    parser.add_argument('--clip_num', type=float, default=0.0)\n",
    "    parser.add_argument('--cuda', type=int, default=1)\n",
    "    parser.add_argument('--order', type=int, default=3)\n",
    "    parser.add_argument('--dp', type=float, default=0.8)\n",
    "    parser.add_argument('--n_hid', type=int, default=64)\n",
    "    parser.add_argument('--use_bias', type=bool, default=True)\n",
    "    parser.add_argument('--top_k', type=int, default=10)\n",
    "    parser.add_argument('--val_prop', type=float, default=0.1)\n",
    "    parser.add_argument('--batch_size', type=int, default=4)\n",
    "    parser.add_argument('--k_job', type=int, default=500)\n",
    "    parser.add_argument('--k_person', type=int, default=2000)\n",
    "    parser.add_argument('--seed', type=int, default=101)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--n_epoch', type=int, default=10000)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0001)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--stepsize', type=int, default=1000)\n",
    "    parser.add_argument('--beta_s', type=float, default=0.4)\n",
    "    parser.add_argument('--beta_e', type=float, default=0.9999)\n",
    "    args = parser.parse_args(args=[])\n",
    "    # print('args:', args)\n",
    "    \n",
    "    args.device = torch.device(\"cpu\")\n",
    "    if args.cuda >= 0:\n",
    "        args.device = torch.device(\"cuda:\" + str(args.cuda))\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e68fa6-58a1-44e4-8142-2dc89355a0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327eb972-24ec-42af-b9a6-36bffdeff83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
